# TraceFlow Lite

[![CI](https://github.com/khalilCodeX/traceflow-lite/actions/workflows/ci.yml/badge.svg)](https://github.com/khalilCodeX/traceflow-lite/actions/workflows/ci.yml)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)

**A control plane that makes agentic systems shippable: tracing + eval gates + fallback + budgets + replay.**
**A production-grade SDK for building reliable, observable AI agent workflows.**

TraceFlow Lite wraps your LLM calls with enterprise-ready reliability mechanisms: eval gates, cost/latency constraints, provider abstraction, trace persistence, and replay capabilities â€” all orchestrated through LangGraph.

---

## Why TraceFlow Lite?

Building reliable AI agents is hard. You need to handle:
- **Cost blowouts** â€” A single runaway query can drain your budget
- **Latency spikes** â€” Users abandon slow responses
- **Quality inconsistency** â€” LLMs hallucinate and go off-topic
- **Debugging nightmares** â€” "What prompt caused that output?"
- **Provider lock-in** â€” Switching from OpenAI to Anthropic shouldn't require rewrites

TraceFlow Lite solves these by providing a **control plane** that sits between your application and LLM providers.

---

## Features

| Feature | Description |
|---------|-------------|
| ğŸ”„ **LangGraph Orchestration** | Multi-step agent workflow with conditional routing and revision loops |
| ğŸ›¡ï¸ **Eval Gates** | Automatic cost, latency, and quality checks before responses are finalized |
| ğŸ’° **Cost Tracking** | Per-request token counting via tiktoken with USD cost calculation |
| ğŸ” **Retry & Revision** | Tenacity-powered retries + intelligent revision loop for quality |
| ğŸ“Š **Trace Persistence** | SQLite storage (WAL mode) for debugging, analytics, and replay |
| ğŸ”Œ **Pluggable Retriever** | Bring your own RAG with flexible callback interface |
| ğŸ­ **Provider Abstraction** | Easily swap or add LLM providers without code changes |

---

## Architecture

![TraceFlow Lite Architecture](./Architecture/traceflow-workflow.png)

### Workflow Overview

1. **Client** receives a user query and configuration
2. **Intake Node** extracts and validates the input
3. **Planner Node** decides if context retrieval is needed
4. **Retriever Node** (optional) fetches relevant documents via your RAG callback
5. **Executor Node** calls the LLM provider with retry logic
6. **Evaluator Node** checks cost/latency constraints and quality
7. **Router Node** directs traffic based on eval decision:
   - `PASS` â†’ Return final answer
   - `REVISE` â†’ Loop back to executor with refinement instructions
   - `FALLBACK` â†’ Return graceful fallback message

All interactions are traced to SQLite for debugging and replay.

---

## Installation

### Using Poetry (recommended)

```bash
git clone https://github.com/khalilCodeX/traceflow-lite.git
cd traceflow-lite
poetry install
```

### Using pip

```bash
git clone https://github.com/khalilCodeX/traceflow-lite.git
cd traceflow-lite
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Development Setup

```bash
# Install with dev dependencies
poetry install --with dev

# Run tests
poetry run pytest

# Run linter
poetry run ruff check .
```

### Adding Dependencies

```bash
# Add a runtime dependency
poetry add <package>

# Add a dev dependency
poetry add --group dev <package>

# After adding, sync requirements.txt for Streamlit Cloud:
poetry export --without-hashes -o requirements.txt
```

---

## Quick Start

```python
from client import TraceFlowClient
from tf_types import RunConfig, Mode

client = TraceFlowClient()

# Basic usage with defaults
result = client.run("What is machine learning?")
print(result.answer)

# With custom configuration
config = RunConfig(
    mode=Mode.GROUNDED_QA,
    model="gpt-3.5-turbo",
    max_tokens=500,
    max_cost_usd=0.10,      # Budget limit
    max_latency_ms=10000,   # Latency limit
    max_revisions=2         # Max retry attempts
)
result = client.run("Explain neural networks", config)

print(f"Answer: {result.answer}")
print(f"Status: {result.status}")
print(f"Trace ID: {result.trace_id}")
```

---

## Using a Custom Retriever (RAG)

TraceFlow Lite doesn't lock you into a specific vector database. Provide your own retriever function:

```python
from tf_types import RunConfig, RetrievedChunk

def my_retriever(query: str) -> list[RetrievedChunk]:
    # Your Chroma/Pinecone/Weaviate/custom implementation
    return [
        RetrievedChunk(
            chunk_id="doc_1",
            content="Relevant context here...",
            source="knowledge_base",
            relevance_score=0.95
        )
    ]

config = RunConfig(retriever_fn=my_retriever)
result = client.run("Question needing context", config)
```

### Using the Built-in Chroma Helper

```python
from utils.retriever_utils import chroma_retriever
from utils.vector_types import chroma_params

# Setup retriever with your documents
documents = [
    "AI is the simulation of human intelligence by machines.",
    "Machine learning is a subset of AI that learns from data.",
]

params = chroma_params(
    documents=documents,
    collection="my_knowledge_base",
    directory="./chroma_db"
)

retriever = chroma_retriever(local=True, params=params)
retriever.create_vector_store(documents)

# Use in your runs
config = RunConfig(retriever_fn=retriever.retrieve_similar_docs)
result = client.run("What is AI?", config)
```

---

## Trace Persistence & Replay

Every run is automatically persisted. Debug issues or experiment with different configs:

```python
# List recent traces
traces = client.list_traces(limit=10)
for trace in traces:
    print(f"{trace.trace_id}: {trace.user_input[:50]}...")

# Get a specific trace
trace = client.get_trace("abc123...")
print(trace.final_answer)

# Replay with different configuration
result = client.replay(
    trace_id="abc123...",
    overrides=RunConfig(model="gpt-4o", max_tokens=1000)
)
```

---

## Configuration Reference

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `mode` | `Mode` | `GROUNDED_QA` | Workflow mode (GROUNDED_QA, TRIAGE_PLAN, CHANGE_SAFETY) |
| `model` | `str` | `gpt-3.5-turbo` | LLM model identifier |
| `provider` | `str` | `openai` | Provider name |
| `max_tokens` | `int` | `1024` | Max output tokens |
| `temperature` | `float` | `0.2` | Sampling temperature |
| `max_cost_usd` | `float` | `1.50` | Budget limit per run (USD) |
| `max_latency_ms` | `int` | `30000` | Latency limit (milliseconds) |
| `max_revisions` | `int` | `3` | Max revision attempts before fallback |
| `strictness` | `Strictness` | `BALANCED` | Eval gate strictness (LENIENT, BALANCED, STRICT) |
| `retriever_fn` | `Callable` | `None` | Custom retriever callback for RAG |
| `enable_cache` | `bool` | `True` | Enable LLM response caching |

---

## Project Structure

```
traceflow-lite/
â”œâ”€â”€ client.py                 # Public SDK entrypoint
â”œâ”€â”€ tf_types.py               # Types, enums, dataclasses
â”œâ”€â”€ state.py                  # Pydantic state models
â”‚
â”œâ”€â”€ graph_flow/
â”‚   â”œâ”€â”€ graph.py              # LangGraph workflow definition
â”‚   â””â”€â”€ nodes/
â”‚       â””â”€â”€ nodes.py          # Node implementations
â”‚
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ base.py               # BaseProvider ABC
â”‚   â”œâ”€â”€ openai_provider.py    # OpenAI implementation
â”‚   â”œâ”€â”€ router.py             # Provider factory
â”‚   â”œâ”€â”€ cost.py               # Token counting & pricing
â”‚   â””â”€â”€ retry.py              # Tenacity retry decorator
â”‚
â”œâ”€â”€ persistence/
â”‚   â”œâ”€â”€ sqlite.py             # DB connection & schema
â”‚   â””â”€â”€ trace_store.py        # CRUD operations
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ retriever_utils.py    # Chroma helper
â”‚   â””â”€â”€ vector_types.py       # Retriever config types
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_client.py        # Integration tests
```

---

## Environment Variables

Create a `.env` file in your project root:

```env
OPENAI_API_KEY=sk-...

# Optional for Chroma Cloud:
CHROMA_API_KEY=...
CHROMA_TENANT=...
CHROMA_DATABASE=...
```

---

## Running Tests

```bash
# Run all tests
pytest tests/test_client.py -v

# Run specific test
pytest tests/test_client.py::test_basic_run_without_retriever -v
```

---

## Roadmap

- [x] Multi-provider support (OpenAI, Anthropic)
- [x] Response caching implementation
- [x] Streamlit ops dashboard
- [x] Trace persistence with SQLite + WAL mode
- [x] Eval gate pattern with revision loop
- [x] Cost tracking per request
- [x] CI/CD with GitHub Actions
- [x] Architecture Decision Records (ADRs)
- [ ] CLI tool (`traceflow run "query"`)
- [ ] Budget-aware model fallback
- [ ] Advanced evaluators (relevance scoring, citation validation)
- [ ] Async execution support
- [ ] OpenTelemetry export integration

---

## Contributing

Contributions are welcome! Please read our contributing guidelines and submit PRs.

---

## License

Apache 2.0
